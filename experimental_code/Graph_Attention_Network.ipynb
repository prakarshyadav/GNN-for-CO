{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph-Attention-Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salujajustin/GNN-for-CO/blob/main/experimental_code/Graph_Attention_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2psz4OYutIDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccde8a8e-6797-4b62-d5a3-2717cf9715e5"
      },
      "source": [
        "# Install required packages, set backend\n",
        "!pip install -q dgl         # For CPU Build\n",
        "!pip install -q dgl-cu101   # For CUDA 10.1 Build"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4MB 18.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 36.2MB 85kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyWxazG2tYi7",
        "outputId": "b3b5d56a-0551-4169-8f23-115ebdbaaa77"
      },
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import pdb\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "import dgl\n",
        "from dgl.nn.pytorch import GATConv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JdhC5w1qDLl"
      },
      "source": [
        "\\begin{equation}\n",
        "z_i^{(l)}=W^{(l)}h_i^{(l)}\n",
        "\\tag{1}\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "e_{ij}^{(l)}=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}|z_j^{(l)}))\n",
        "\\tag{2}\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_{ij}^{(l)}=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})} \n",
        "\\tag{3}\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "h_i^{(l+1)}=\\sigma \\left(\\sum_{j \\in \\mathcal{N}(i)}  {\\alpha^{(l)}_{ij} z^{(l)}_j} \\right)\n",
        "\\tag{4}\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWRejwePt54B"
      },
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim, dropout=0.6):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.g = g                                                  # graph\n",
        "        self.attn_dropout = dropout                                 # dropout\n",
        "        self.feat_dropout = dropout\n",
        "        self.fc = nn.Linear(in_dim, out_dim, bias=False)            # equation (1)\n",
        "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)        # equation (2)\n",
        "        \n",
        "        # Initialize learnable parameters\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "\n",
        "    def edge_attention(self, edges):                                # Edge UDF: equation (2)\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "    def message_func(self, edges):                                  # Message UDF: equation (3),(4)\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "    def reduce_func(self, nodes):                                   # Reduce UDF: equation (3),(4)\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)                # equation (3)\n",
        "        # alpha = F.dropout(alpha, self.dropout, self.training)       # dropout\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)            # equation (4)\n",
        "        return {'h': h}\n",
        "\n",
        "    def forward(self, h):\n",
        "        z = self.fc(h)                                              # equation (1)\n",
        "        # z = F.dropout(z, self.dropout, self.training)               # dropout\n",
        "        self.g.ndata['z'] = z\n",
        "        self.g.apply_edges(self.edge_attention)                     # equation (2)\n",
        "        self.g.update_all(self.message_func, self.reduce_func)      # equation (3),(4)\n",
        "        return self.g.ndata.pop('h')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pezhzPkMqDLs"
      },
      "source": [
        "class MultiHeadGATLayer(nn.Module):\n",
        "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
        "        super(MultiHeadGATLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
        "        self.merge = merge\n",
        "\n",
        "    def forward(self, h):\n",
        "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
        "        if self.merge == 'cat':\n",
        "            # concat on the output feature dimension (dim=1)\n",
        "            return torch.cat(head_outs, dim=1)\n",
        "        else:\n",
        "            # merge using average\n",
        "            return torch.mean(torch.stack(head_outs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06jnfPFpqDLt"
      },
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
        "        super(GAT, self).__init__()\n",
        "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
        "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
        "        # multiple head outputs are concatenated together. Also, only\n",
        "        # one attention head in the output layer.\n",
        "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = inputs\n",
        "        h = self.layer1(h)\n",
        "        h = F.elu(h)\n",
        "        h = self.layer2(h)\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB8LaZ4lFsOT"
      },
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 g,\n",
        "                 num_layers,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 num_classes,\n",
        "                 heads,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 negative_slope,\n",
        "                 residual):\n",
        "        super(GAT, self).__init__()\n",
        "        self.g = g\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATConv(\n",
        "            in_dim, num_hidden, heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATConv(\n",
        "            num_hidden * heads[-2], num_classes, heads[-1],\n",
        "            feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](self.g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](self.g, h).mean(1)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RusGLBF84enk"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOiRB0Ha1wlv"
      },
      "source": [
        "def accuracy(logits, labels):\n",
        "    indices = torch.argmax(logits, dim=1)               # indices with highest value\n",
        "    num_correct = torch.sum(indices == labels)          # how many predictions match labels\n",
        "    return (num_correct.item()*1.0)/len(labels)         # convert to float and find percentage \n",
        "\n",
        "def evaluate(model, features, labels, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():                               # deactivate autograd during eval\n",
        "        logits = model(features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        return accuracy(logits, labels)\n",
        "\n",
        "def train(model, features, labels, mask):\n",
        "    model.train()\n",
        "    print(labels.size(),features.size())\n",
        "    logits = model(features)\n",
        "    logp = F.log_softmax(logits, 1)\n",
        "    loss = F.nll_loss(logp[mask], labels[mask])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k-mSdd4h4K"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbsAXGm1qDLw",
        "outputId": "cf0137f6-30b8-4e64-c002-c9301806b970"
      },
      "source": [
        "# Cora dataset consists of 2708 publications classified into one of seven classes\n",
        "# Each publication is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary\n",
        "dataset = dgl.data.CoraGraphDataset()\n",
        "\n",
        "# Check is GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset and attributes\n",
        "graph = dataset[0]                                  # Only 1 graph in this dataset\n",
        "graph = graph.int().to(device)                      # Cast to GPU if available, else cpu\n",
        "node_features = graph.ndata['feat']                 # [2708, 1433]: each node has a word vector of 1433 unique words\n",
        "node_labels = graph.ndata['label']                  # [2708]: each node has one label of range [0-6]\n",
        "train_mask = graph.ndata['train_mask']\n",
        "valid_mask = graph.ndata['val_mask']\n",
        "test_mask = graph.ndata['test_mask']\n",
        "num_feats = node_features.size()[1]\n",
        "num_classes = dataset.num_classes\n",
        "\n",
        "# GAT Hyperparameters\n",
        "num_heads = 8\n",
        "num_layers = 1\n",
        "num_out_heads = 1\n",
        "\n",
        "heads = ([num_heads] * num_layers) + [num_out_heads]\n",
        "model = GAT(g=graph, num_layers=num_layers, in_dim=num_feats, num_hidden=8, num_classes=num_classes, heads=heads, activation=F.elu, feat_drop=0.6, attn_drop=0.6, negative_slope=0.2, residual=False)\n",
        "\n",
        "# Old model\n",
        "# model = GAT(graph, in_dim=num_feats, hidden_dim=8, out_dim=num_classes, num_heads=8)\n",
        "\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=5e-4)\n",
        "\n",
        "# Main\n",
        "for epoch in range(300):\n",
        "\n",
        "    loss = train(model, node_features, node_labels, train_mask)\n",
        "    val_acc = evaluate(model, node_features, node_labels, valid_mask)\n",
        "\n",
        "    # if epoch % 10 == 9:\n",
        "    print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f}\".format(epoch+1, loss, val_acc))\n",
        "\n",
        "# Testing\n",
        "test_acc = evaluate(model, node_features, node_labels, test_mask)\n",
        "print(\"Test Accuracy {:.4f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00001 | Loss 1.9457 | Accuracy 0.2420\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00002 | Loss 1.9351 | Accuracy 0.2620\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00003 | Loss 1.9296 | Accuracy 0.2340\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00004 | Loss 1.9210 | Accuracy 0.3200\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00005 | Loss 1.9140 | Accuracy 0.3800\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00006 | Loss 1.9095 | Accuracy 0.3160\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00007 | Loss 1.8954 | Accuracy 0.3460\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00008 | Loss 1.8773 | Accuracy 0.3520\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00009 | Loss 1.8887 | Accuracy 0.4200\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00010 | Loss 1.8696 | Accuracy 0.4780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00011 | Loss 1.8782 | Accuracy 0.5520\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00012 | Loss 1.8522 | Accuracy 0.6140\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00013 | Loss 1.8499 | Accuracy 0.6620\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00014 | Loss 1.8379 | Accuracy 0.7060\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00015 | Loss 1.8431 | Accuracy 0.7360\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00016 | Loss 1.8160 | Accuracy 0.7540\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00017 | Loss 1.8205 | Accuracy 0.7720\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00018 | Loss 1.7916 | Accuracy 0.7680\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00019 | Loss 1.8053 | Accuracy 0.7740\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00020 | Loss 1.8102 | Accuracy 0.7780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00021 | Loss 1.7941 | Accuracy 0.7820\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00022 | Loss 1.7657 | Accuracy 0.7760\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00023 | Loss 1.7585 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00024 | Loss 1.7587 | Accuracy 0.7800\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00025 | Loss 1.7308 | Accuracy 0.7740\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00026 | Loss 1.7561 | Accuracy 0.7660\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00027 | Loss 1.7208 | Accuracy 0.7740\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00028 | Loss 1.6975 | Accuracy 0.7660\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00029 | Loss 1.6804 | Accuracy 0.7720\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00030 | Loss 1.6847 | Accuracy 0.7760\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00031 | Loss 1.6965 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00032 | Loss 1.6969 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00033 | Loss 1.6867 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00034 | Loss 1.6284 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00035 | Loss 1.6798 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00036 | Loss 1.6064 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00037 | Loss 1.6229 | Accuracy 0.7720\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00038 | Loss 1.6045 | Accuracy 0.7700\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00039 | Loss 1.6071 | Accuracy 0.7660\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00040 | Loss 1.5900 | Accuracy 0.7640\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00041 | Loss 1.5351 | Accuracy 0.7580\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00042 | Loss 1.5418 | Accuracy 0.7620\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00043 | Loss 1.5386 | Accuracy 0.7680\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00044 | Loss 1.5915 | Accuracy 0.7780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00045 | Loss 1.5082 | Accuracy 0.7820\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00046 | Loss 1.5148 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00047 | Loss 1.5390 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00048 | Loss 1.4316 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00049 | Loss 1.5330 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00050 | Loss 1.5034 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00051 | Loss 1.4267 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00052 | Loss 1.4674 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00053 | Loss 1.4078 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00054 | Loss 1.3759 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00055 | Loss 1.3720 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00056 | Loss 1.4231 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00057 | Loss 1.3886 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00058 | Loss 1.4237 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00059 | Loss 1.4512 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00060 | Loss 1.3634 | Accuracy 0.7800\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00061 | Loss 1.3072 | Accuracy 0.7780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00062 | Loss 1.3965 | Accuracy 0.7780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00063 | Loss 1.3598 | Accuracy 0.7780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00064 | Loss 1.4666 | Accuracy 0.7740\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00065 | Loss 1.3553 | Accuracy 0.7760\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00066 | Loss 1.3094 | Accuracy 0.7800\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00067 | Loss 1.4153 | Accuracy 0.7820\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00068 | Loss 1.2756 | Accuracy 0.7780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00069 | Loss 1.4621 | Accuracy 0.7800\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00070 | Loss 1.2596 | Accuracy 0.7820\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00071 | Loss 1.2650 | Accuracy 0.7780\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00072 | Loss 1.2968 | Accuracy 0.7800\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00073 | Loss 1.2578 | Accuracy 0.7800\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00074 | Loss 1.2980 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00075 | Loss 1.3179 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00076 | Loss 1.2589 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00077 | Loss 1.2557 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00078 | Loss 1.2190 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00079 | Loss 1.1970 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00080 | Loss 1.2223 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00081 | Loss 1.2316 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00082 | Loss 1.2665 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00083 | Loss 1.1940 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00084 | Loss 1.2828 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00085 | Loss 1.2417 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00086 | Loss 1.2984 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00087 | Loss 1.0864 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00088 | Loss 1.0847 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00089 | Loss 1.1524 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00090 | Loss 1.2169 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00091 | Loss 1.2167 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00092 | Loss 1.1879 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00093 | Loss 1.1158 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00094 | Loss 1.0834 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00095 | Loss 1.1488 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00096 | Loss 1.1658 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00097 | Loss 1.1182 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00098 | Loss 1.0756 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00099 | Loss 1.1420 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00100 | Loss 1.1186 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00101 | Loss 1.0835 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00102 | Loss 1.1108 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00103 | Loss 1.1406 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00104 | Loss 1.0207 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00105 | Loss 1.0597 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00106 | Loss 1.0178 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00107 | Loss 1.0117 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00108 | Loss 1.1029 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00109 | Loss 1.0426 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00110 | Loss 1.0664 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00111 | Loss 1.0279 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00112 | Loss 0.9395 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00113 | Loss 1.0339 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00114 | Loss 1.0887 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00115 | Loss 1.0863 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00116 | Loss 1.1291 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00117 | Loss 0.9576 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00118 | Loss 1.0562 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00119 | Loss 1.0911 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00120 | Loss 1.0149 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00121 | Loss 1.0491 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00122 | Loss 0.9157 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00123 | Loss 1.0313 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00124 | Loss 1.0548 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00125 | Loss 1.0211 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00126 | Loss 1.0494 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00127 | Loss 0.9544 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00128 | Loss 1.0088 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00129 | Loss 0.9791 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00130 | Loss 0.9921 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00131 | Loss 0.9587 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00132 | Loss 0.9746 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00133 | Loss 1.0683 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00134 | Loss 1.0365 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00135 | Loss 0.9217 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00136 | Loss 1.0662 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00137 | Loss 0.9638 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00138 | Loss 1.0584 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00139 | Loss 0.9858 | Accuracy 0.8100\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00140 | Loss 1.0271 | Accuracy 0.8100\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00141 | Loss 1.0152 | Accuracy 0.8100\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00142 | Loss 1.0869 | Accuracy 0.8060\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00143 | Loss 0.9177 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00144 | Loss 1.0660 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00145 | Loss 1.0354 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00146 | Loss 0.9077 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00147 | Loss 0.9921 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00148 | Loss 0.9746 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00149 | Loss 0.8971 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00150 | Loss 0.9468 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00151 | Loss 1.0059 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00152 | Loss 1.0117 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00153 | Loss 0.9138 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00154 | Loss 0.9619 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00155 | Loss 1.0685 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00156 | Loss 0.9480 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00157 | Loss 0.9440 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00158 | Loss 1.0065 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00159 | Loss 0.9495 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00160 | Loss 0.9558 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00161 | Loss 0.9499 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00162 | Loss 0.9038 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00163 | Loss 1.0131 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00164 | Loss 0.9970 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00165 | Loss 0.8961 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00166 | Loss 0.9429 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00167 | Loss 0.9474 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00168 | Loss 0.9953 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00169 | Loss 0.9562 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00170 | Loss 0.7831 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00171 | Loss 0.9035 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00172 | Loss 0.8721 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00173 | Loss 0.8710 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00174 | Loss 0.9606 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00175 | Loss 0.8660 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00176 | Loss 0.9336 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00177 | Loss 0.9273 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00178 | Loss 0.9396 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00179 | Loss 1.0665 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00180 | Loss 0.9311 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00181 | Loss 0.8204 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00182 | Loss 0.9203 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00183 | Loss 0.8422 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00184 | Loss 0.9937 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00185 | Loss 1.0140 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00186 | Loss 0.7739 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00187 | Loss 0.9910 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00188 | Loss 0.9423 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00189 | Loss 0.9633 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00190 | Loss 0.8641 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00191 | Loss 0.9202 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00192 | Loss 0.9256 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00193 | Loss 0.9937 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00194 | Loss 0.8209 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00195 | Loss 0.9356 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00196 | Loss 0.9197 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00197 | Loss 0.9386 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00198 | Loss 0.9305 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00199 | Loss 0.8190 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00200 | Loss 0.9777 | Accuracy 0.8040\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00201 | Loss 0.8657 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00202 | Loss 0.7664 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00203 | Loss 0.9538 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00204 | Loss 0.8336 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00205 | Loss 0.8883 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00206 | Loss 0.8623 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00207 | Loss 0.9147 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00208 | Loss 0.8731 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00209 | Loss 0.8423 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00210 | Loss 0.8078 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00211 | Loss 0.8928 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00212 | Loss 0.8378 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00213 | Loss 0.9243 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00214 | Loss 0.8034 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00215 | Loss 0.8207 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00216 | Loss 0.9876 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00217 | Loss 0.8856 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00218 | Loss 0.9029 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00219 | Loss 0.9753 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00220 | Loss 0.8345 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00221 | Loss 1.0089 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00222 | Loss 0.9367 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00223 | Loss 0.8634 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00224 | Loss 0.7853 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00225 | Loss 0.8984 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00226 | Loss 0.8889 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00227 | Loss 0.7936 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00228 | Loss 0.8836 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00229 | Loss 0.8713 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00230 | Loss 0.8254 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00231 | Loss 0.8381 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00232 | Loss 0.8839 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00233 | Loss 0.8769 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00234 | Loss 0.8449 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00235 | Loss 0.8596 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00236 | Loss 0.7940 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00237 | Loss 0.7825 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00238 | Loss 0.8535 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00239 | Loss 0.7622 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00240 | Loss 0.8756 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00241 | Loss 0.8940 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00242 | Loss 0.7177 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00243 | Loss 0.7594 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00244 | Loss 0.8319 | Accuracy 0.8060\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00245 | Loss 0.8381 | Accuracy 0.8020\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00246 | Loss 0.9709 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00247 | Loss 0.7845 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00248 | Loss 0.8605 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00249 | Loss 0.7769 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00250 | Loss 0.8247 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00251 | Loss 0.8409 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00252 | Loss 0.8203 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00253 | Loss 0.8244 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00254 | Loss 0.8492 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00255 | Loss 0.8838 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00256 | Loss 0.8318 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00257 | Loss 0.8091 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00258 | Loss 0.7826 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00259 | Loss 0.8338 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00260 | Loss 0.8581 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00261 | Loss 0.7667 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00262 | Loss 0.8425 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00263 | Loss 0.8063 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00264 | Loss 0.8114 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00265 | Loss 0.8102 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00266 | Loss 0.9365 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00267 | Loss 0.8457 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00268 | Loss 0.9570 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00269 | Loss 0.8128 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00270 | Loss 0.6859 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00271 | Loss 0.8323 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00272 | Loss 0.9752 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00273 | Loss 0.8241 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00274 | Loss 0.9172 | Accuracy 0.7820\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00275 | Loss 0.8494 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00276 | Loss 0.8385 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00277 | Loss 0.9288 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00278 | Loss 0.8837 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00279 | Loss 0.7905 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00280 | Loss 0.6802 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00281 | Loss 0.8691 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00282 | Loss 0.7628 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00283 | Loss 0.8242 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00284 | Loss 0.8592 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00285 | Loss 0.9469 | Accuracy 0.8000\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00286 | Loss 0.8075 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00287 | Loss 0.8246 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00288 | Loss 0.7370 | Accuracy 0.7980\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00289 | Loss 0.8449 | Accuracy 0.7940\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00290 | Loss 0.7928 | Accuracy 0.7960\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00291 | Loss 0.8823 | Accuracy 0.7920\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00292 | Loss 0.8029 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00293 | Loss 0.7553 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00294 | Loss 0.8869 | Accuracy 0.7900\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00295 | Loss 0.7940 | Accuracy 0.7880\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00296 | Loss 0.7419 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00297 | Loss 0.8560 | Accuracy 0.7860\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00298 | Loss 0.7702 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00299 | Loss 0.8492 | Accuracy 0.7840\n",
            "torch.Size([2708]) torch.Size([2708, 1433])\n",
            "Epoch 00300 | Loss 0.9809 | Accuracy 0.7840\n",
            "Test Accuracy 0.8150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxWr6H9rKBjC"
      },
      "source": [
        "#### Misc testing below (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-TgRDp77M0Y",
        "outputId": "57d6660a-44e6-44a0-8881-50dd3eb0f996"
      },
      "source": [
        "print(graph)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph(num_nodes=2708, num_edges=10556,\n",
            "      ndata_schemes={'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'label': Scheme(shape=(), dtype=torch.int64), 'feat': Scheme(shape=(1433,), dtype=torch.float32)}\n",
            "      edata_schemes={})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "ySFtiYLf8CNj",
        "outputId": "9ba2dde4-c6a8-47c1-fc1c-422412f185c6"
      },
      "source": [
        "print(labels.size())\n",
        "print(max(labels))\n",
        "print(min(labels))\n",
        "print(labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bcbbe2859815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edug7y1j8_C5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "from dgl.nn import GATConv\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 g,\n",
        "                 num_layers,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 num_classes,\n",
        "                 heads,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 negative_slope,\n",
        "                 residual):\n",
        "        super(GAT, self).__init__()\n",
        "        self.g = g\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATConv(\n",
        "            in_dim, num_hidden, heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATConv(\n",
        "            num_hidden * heads[-2], num_classes, heads[-1],\n",
        "            feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](self.g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.gat_layers[-1](self.g, h).mean(1)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhUJiIoT1Zq0"
      },
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def step(self, acc, model):\n",
        "        score = acc\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        torch.save(model.state_dict(), 'es_checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}